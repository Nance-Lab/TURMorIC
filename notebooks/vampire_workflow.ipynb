{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85cc97a1",
   "metadata": {},
   "source": [
    "# 1. Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c70a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import turmoric\n",
    "from turmoric.image_process import nd2_to_tif\n",
    "from turmoric.utils import recursively_get_all_filepaths\n",
    "from turmoric.apply_thresholds import apply_all_thresh\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tifffile as tiff\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from skimage import filters, io\n",
    "from skimage.segmentation import clear_border\n",
    "from skimage.morphology import remove_small_objects\n",
    "from skimage.measure import block_reduce, label, regionprops\n",
    "from skimage import exposure\n",
    "from scipy import ndimage\n",
    "\n",
    "from turmoric.cell_analysis import apply_regionprops_recursively\n",
    "from turmoric.utils import organize_files_without_leakage\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "\n",
    "import vampire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7857907",
   "metadata": {},
   "source": [
    "### Making sure your images are organized properly will be really helpful to running the code successfully. It should look like this:\n",
    "\n",
    "```\n",
    "~/Data\n",
    "  ├── region_1\n",
    "      ├── treatment_1\n",
    "      ├── treatment_2\n",
    "      └── treatment_3\n",
    "  ├── region_2\n",
    "      ├── treatment_1\n",
    "      ├── treatment_2\n",
    "      └── treatment_3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da6f546",
   "metadata": {},
   "source": [
    "# 2. If images are saved as ND2 files, convert to TIFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71aa967",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_nd2s = \"/Users/nelsschimek/Documents/nancelab/Data/110225_inhibitors_bv-2_sheets/\" # Put in the path to the directory that holds your images here\n",
    "\n",
    "\n",
    "file_list = recursively_get_all_filepaths(path_to_nd2s, file_type=\"nd2\")\n",
    "\n",
    "for file in file_list:\n",
    "\n",
    "    path_name = Path(file)  \n",
    "    file_name = os.path.basename(file)\n",
    "    nd2_to_tif(path_name.parent, file_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b5b5bb",
   "metadata": {},
   "source": [
    "# 3. Determine the best thresholding method\n",
    "\n",
    "#### This section is possibly the most important part of this entire process, as it determines the quality of data used for analysis. We will be tuning a few different parameters, namely:\n",
    "\n",
    "1. The threshold method being used\n",
    "2. the minimum size for excluding objects in the image, which are likely imaging artifacts and not cells\n",
    "3. The maximum size for exluding objects from the image; large objects are likely overlapping cells that the thresholding method cannot correctly seperate into multiple objects\n",
    "\n",
    "#### The goal of this process is to minimize the background in the image while including as many real individual cells as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01460e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_tifs = \"/Users/nelsschimek/Documents/nancelab/Data/110225_inhibitors_bv-2_sheets/control-24h\"\n",
    "tif_files = recursively_get_all_filepaths(path_to_tifs, file_type='tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac961e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "tif_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52499114",
   "metadata": {},
   "source": [
    "#### First, we will try a few different thresholding methods to see if any are clearly are best at seperating out cells. You must also tell the function which channel of the image the cells of interest are in. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a7f6b3",
   "metadata": {},
   "source": [
    "#### After looking through the images, there should be 1-3 thresholds that are best at capturing the cells in the original image. We will now look at the minimize and maximum object size parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7147a14",
   "metadata": {},
   "source": [
    "#### There are some parameters that you can tune in the ***create_microglia*** function, specifically:\n",
    "\n",
    "1. max_obj_size (default is 50,000). Any detected objects that are more than max_obj_size pixels in area will be removed by the threshold. This is particularly useful if the threshold algorithm cannot seperate clusters of cells into individual objects\n",
    "\n",
    "2. min_obj_size (default is 250). Any detected objects that have an area of less than min_obj_size pixels will be removed by the threshold. This is useful when there is background signal that the threshold detects as an object but doesn't correspond to a cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472cb792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_microglia_mask(image, threshold_method=filters.threshold_li, max_obj_size=50000, min_obj_size=250):\n",
    "    print(image.shape)\n",
    "    image = image[1,:,:] # channel that cells of interest\n",
    "    print(image.shape)\n",
    "\n",
    "    thresh_li = threshold_method(image)\n",
    "    binary_li = image > thresh_li\n",
    "\n",
    "    objects = label(binary_li)\n",
    "    objects = clear_border(objects)\n",
    "    large_objects = remove_small_objects(objects, min_size=max_obj_size)\n",
    "    small_objects = label((objects ^ large_objects) > thresh_li)\n",
    "\n",
    "    binary_li = ndimage.binary_fill_holes(remove_small_objects(small_objects > thresh_li, min_size=min_obj_size))\n",
    "\n",
    "    return binary_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2891244",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_images = [tiff.imread(f) for f in sorted(tif_files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beadf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_masks = [create_microglia_mask(f, threshold_method=filters.threshold_mean) for f in cell_images]\n",
    "li_masks = [create_microglia_mask(f, threshold_method=filters.threshold_li) for f in cell_images]\n",
    "#otsu_masks = [create_microglia_mask(f, threshold_method=filters.threshold_otsu) for f in cell_images]\n",
    "#isodata_masks = [create_microglia_mask(f, threshold_method=filters.threshold_isodata) for f in cell_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68473cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = 3\n",
    "rows = 5#len(li_masks)\n",
    "\n",
    "fig, axes = plt.subplots(len(li_masks), columns, figsize=(20, 50))\n",
    "for ax, original, li, mean in zip(axes, cell_images, li_masks, mean_masks):\n",
    "\n",
    "    raw_image = original[1,:,:]\n",
    "    enhanced_im = exposure.equalize_adapthist(raw_image, clip_limit=0.03)\n",
    "    ax[0].imshow(enhanced_im, cmap=\"gray\")\n",
    "    ax[1].imshow(mean, cmap=\"gray\")\n",
    "    ax[2].imshow(li, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ce3a86",
   "metadata": {},
   "source": [
    "# 4. Applying chosen threshold to all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0758d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_li_threshold(input_folder, output_folder, channel, size):\n",
    "    \"\"\"\n",
    "    Applies Li thresholding to all .tif images in the input folder\n",
    "    (and subfolders)\n",
    "    and saves the binary masks in the output folder.\n",
    "\n",
    "    Parameters:\n",
    "    - input_folder: Path to the folder containing .tif images.\n",
    "    - output_folder: Path to save the processed binary masks.\n",
    "    - size: Minimum size of objects to retain in the binary mask.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(input_folder):\n",
    "        print(f\"Error: Input folder '{input_folder}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Walk through all files and subfolders\n",
    "    for root, _, files in os.walk(input_folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\".tif\"):\n",
    "                # Full input path\n",
    "                input_path = os.path.join(root, file)\n",
    "\n",
    "                # Create corresponding output subfolder\n",
    "                relative_path = os.path.relpath(root, input_folder)\n",
    "                output_subfolder = os.path.join(output_folder, relative_path)\n",
    "                os.makedirs(output_subfolder, exist_ok=True)\n",
    "\n",
    "                # Full output path\n",
    "                output_path = os.path.join(output_subfolder,\n",
    "                                           file.replace(\".tif\",\n",
    "                                                        \"_li_thresh.npy\"))\n",
    "\n",
    "                try:\n",
    "                    # Read the image\n",
    "                    img = io.imread(input_path)\n",
    "\n",
    "                    # Assume the second channel is the microglia channel\n",
    "                    # if img.ndim == 3:\n",
    "                    #     microglia_im = img[:, :, 1]\n",
    "                    # elif img.ndim == 2:\n",
    "                    #     microglia_im = img[1, :, :]\n",
    "                    # else:\n",
    "                    #     microglia_im = img\n",
    "\n",
    "                    microglia_im = img[channel, :, :]\n",
    "\n",
    "                    # Apply Li threshold\n",
    "\n",
    "\n",
    "                    # img = tiff.imread(input_path)\n",
    "                    \n",
    "                    binary_li = create_microglia_mask(microglia_im)\n",
    "\n",
    "                        # Save the binary mask as .npy\n",
    "                    np.save(output_path, binary_li)\n",
    "                        \n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {input_path}: {e}\")\n",
    "\n",
    "    print(f\"Processing completed. Results are saved in '{output_folder}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c663e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"\"\n",
    "thresholded_images = \"/Users/nelsschimek/Documents/nancelab/Data/gaby_data/tifs/li_thresh/Opt\"\n",
    "\n",
    "apply_li_threshold(input_folder=input_folder, output_folder=thresholded_images, channel=1, size=73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a842f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "props_list = ('area', 'bbox_area', 'centroid', 'convex_area',\n",
    "              'eccentricity', 'equivalent_diameter', 'euler_number',\n",
    "              'extent', 'filled_area', 'major_axis_length',\n",
    "              'minor_axis_length', 'orientation', 'perimeter', 'solidity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e1530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "regionprops_df = apply_regionprops_recursively(input_folder=thresholded_images, properties_list=props_list)\n",
    "regionprops_df['circularity'] = 4*np.pi*regionprops_df.area/regionprops_df.perimeter**2\n",
    "regionprops_df['aspect_ratio'] = regionprops_df.major_axis_length/regionprops_df.minor_axis_length\n",
    "regionprops_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03503fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "regionprops_df.to_csv(\"gaby_testing_csv.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48fbd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_npy_to_tif_recursive(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Recursively convert all .npy files to .tif files while maintaining directory structure.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dir : str or Path\n",
    "        Root directory containing .npy files\n",
    "    output_dir : str or Path\n",
    "        Root directory where .tif files will be saved\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    int : Number of files converted\n",
    "    \"\"\"\n",
    "    input_path = Path(input_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    \n",
    "    if not input_path.exists():\n",
    "        raise ValueError(f\"Input directory does not exist: {input_dir}\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    converted_count = 0\n",
    "    \n",
    "    # Walk through all subdirectories\n",
    "    for npy_file in input_path.rglob(\"*.npy\"):\n",
    "        # Get relative path from input directory\n",
    "        relative_path = npy_file.relative_to(input_path)\n",
    "        \n",
    "        # Create corresponding output path with .tif extension\n",
    "        output_file = output_path / relative_path.with_suffix('.tif')\n",
    "        \n",
    "        # Create subdirectories in output if they don't exist\n",
    "        output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            # Load numpy array and save as TIFF\n",
    "            data = np.load(npy_file)\n",
    "            Image.fromarray(data).save(output_file)\n",
    "            \n",
    "            converted_count += 1\n",
    "            print(f\"Converted: {relative_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {npy_file}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\nConversion complete! {converted_count} files converted.\")\n",
    "    return converted_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c2b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_npy_to_tif_recursive(input_dir=\"\", output_dir=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814c05c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your base directory and target directories for training and testing\n",
    "base_dir = \"/Users/nelsschimek/Documents/nancelab/Data/gaby_data/tifs/\"\n",
    "train_dir = \"/Users/nelsschimek/Documents/nancelab/Data/gaby_data/tifs/training\"\n",
    "test_dir = \"/Users/nelsschimek/Documents/nancelab/Data/gaby_data/tifs/testing\"\n",
    "\n",
    "# Define a list of subfolder names or patterns to look for\n",
    "treatment_conditions = [\"Acet\", \"Amin\", \"HC\", \"Meth\", \"Opt\"]\n",
    "groups = [\"converted_tiffs\"]\n",
    "\n",
    "# Create training and testing directories if they don't exist\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc1f893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_generic(base_dir, train_dir, test_dir, groups, treatment_conditions, test_size=0.2, split=True):\n",
    "    for group in groups:\n",
    "        for condition in treatment_conditions:\n",
    "            condition_path = os.path.join(base_dir, group, condition)\n",
    "            if not os.path.exists(condition_path):\n",
    "                continue\n",
    "            \n",
    "            print(f'processing {group} {condition}')\n",
    "            print(condition_path)\n",
    "            files = os.listdir(condition_path)\n",
    "            train_slices, test_slices = train_test_split(files, test_size=test_size)\n",
    "            \n",
    "            # Create subdirectories for training and testing\n",
    "            train_subdir = os.path.join(train_dir, group, condition)\n",
    "            test_subdir = os.path.join(test_dir, group, condition)\n",
    "            os.makedirs(train_subdir, exist_ok=True)\n",
    "            os.makedirs(test_subdir, exist_ok=True)\n",
    "            \n",
    "            # Process training files\n",
    "            for file in train_slices:\n",
    "                src_path = os.path.join(condition_path, file)\n",
    "                \n",
    "                if split:\n",
    "                    # Load image and split into 4 quadrants\n",
    "                    img = Image.open(src_path)\n",
    "                    img_array = np.array(img)\n",
    "                    h, w = img_array.shape\n",
    "                    \n",
    "                    # Split into 4 quadrants\n",
    "                    quadrants = [\n",
    "                        img_array[:h//2, :w//2],      # Top-left (quad1)\n",
    "                        img_array[:h//2, w//2:],      # Top-right (quad2)\n",
    "                        img_array[h//2:, :w//2],      # Bottom-left (quad3)\n",
    "                        img_array[h//2:, w//2:]       # Bottom-right (quad4)\n",
    "                    ]\n",
    "                    \n",
    "                    # Save each quadrant\n",
    "                    filename_without_ext = os.path.splitext(file)[0]\n",
    "                    ext = os.path.splitext(file)[1]\n",
    "                    \n",
    "                    for i, quad in enumerate(quadrants, 1):\n",
    "                        new_filename = f\"{filename_without_ext}_quad{i}{ext}\"\n",
    "                        Image.fromarray(quad).save(os.path.join(train_dir, new_filename))\n",
    "                else:\n",
    "                    # Just copy the file as is\n",
    "                    shutil.copy(src_path, os.path.join(train_dir, file))\n",
    "            \n",
    "            # Process test files\n",
    "            for file in test_slices:\n",
    "                src_path = os.path.join(condition_path, file)\n",
    "                \n",
    "                if split:\n",
    "                    # Load image and split into 4 quadrants\n",
    "                    img = Image.open(src_path)\n",
    "                    img_array = np.array(img)\n",
    "                    h, w = img_array.shape\n",
    "                    \n",
    "                    # Split into 4 quadrants\n",
    "                    quadrants = [\n",
    "                        img_array[:h//2, :w//2],      # Top-left (quad1)\n",
    "                        img_array[:h//2, w//2:],      # Top-right (quad2)\n",
    "                        img_array[h//2:, :w//2],      # Bottom-left (quad3)\n",
    "                        img_array[h//2:, w//2:]       # Bottom-right (quad4)\n",
    "                    ]\n",
    "                    \n",
    "                    # Save each quadrant\n",
    "                    filename_without_ext = os.path.splitext(file)[0]\n",
    "                    ext = os.path.splitext(file)[1]\n",
    "                    \n",
    "                    for i, quad in enumerate(quadrants, 1):\n",
    "                        new_filename = f\"{filename_without_ext}_quad{i}{ext}\"\n",
    "                        Image.fromarray(quad).save(os.path.join(test_subdir, new_filename))\n",
    "                else:\n",
    "                    # Just copy the file as is\n",
    "                    shutil.copy(src_path, os.path.join(test_subdir, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62560b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split_generic(base_dir=base_dir,\n",
    "                         train_dir=train_dir,\n",
    "                         test_dir=test_dir,\n",
    "                         groups=groups,\n",
    "                         treatment_conditions=treatment_conditions,\n",
    "                         test_size=0.2, \n",
    "                         split=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55718288",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_set_path = \"/Users/nelsschimek/Documents/nancelab/Data/gaby_data/tifs/training\"\n",
    "\n",
    "vampire.extraction.extract_properties(image_set_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd119a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_info_df = pd.DataFrame({\n",
    "    'img_set_path': [image_set_path],\n",
    "    'output_path': [image_set_path],\n",
    "    'model_name': ['li'],\n",
    "    'num_points': [50],\n",
    "    'num_clusters': [5],\n",
    "    'num_pc': [np.nan]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5162fb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vampire.quickstart.fit_models(build_info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3329b489",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(image_set_path, 'model_li_(50_5_29)__.pickle')\n",
    "vampire_model = vampire.util.read_pickle(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5116481",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "main_path = \"/Users/nelsschimek/Documents/nancelab/Data/gaby_data/tifs/testing/converted_tiffs\"\n",
    "\n",
    "apply_info_df = pd.DataFrame({\n",
    "    'img_set_path': [\n",
    "        f\"{main_path}/Acet\",\n",
    "        f\"{main_path}/Amin\",\n",
    "        f\"{main_path}/HC\",\n",
    "        f\"{main_path}/Meth\",\n",
    "        #f\"{main_path}/Opt\",\n",
    "    \n",
    "    ],\n",
    "    'model_path': [model_path,\n",
    "                   model_path,\n",
    "                   model_path,\n",
    "                   model_path,\n",
    "                   #model_path\n",
    "                   ],\n",
    "                   \n",
    "    'output_path': [\n",
    "        f\"{main_path}/Acet\",\n",
    "        f\"{main_path}/Amin\",\n",
    "        f\"{main_path}/HC\",\n",
    "        f\"{main_path}/Meth\",\n",
    "        #f\"{main_path}/Opt\",\n",
    "       \n",
    "    ],\n",
    "    'img_set_name': [\n",
    "        \"Acet\",\n",
    "        \"Amin\",\n",
    "        \"HC\",\n",
    "        \"Meth\",\n",
    "        #\"Opt\",\n",
    "    ],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc97272",
   "metadata": {},
   "outputs": [],
   "source": [
    "vampire.quickstart.transform_datasets(apply_info_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vampire",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
